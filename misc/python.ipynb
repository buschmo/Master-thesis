{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Switch to correct folder\n",
    "if not \"__path__\" in locals():\n",
    "    __path__ = Path().absolute()\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from utils.datasets import DatasetWordPiece\n",
    "from models.tvae_trainer import TVAETrainer\n",
    "from models.tvae_model import TVAE\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Dies ist ein total ausgereifter Test, welchem bestimmt nichts kaputt geht.\"\n",
    "\n",
    "model= \"deepset/gbert-base\"\n",
    "tokenizer= BertTokenizer.from_pretrained(model)\n",
    "\n",
    "encoding = tokenizer.encode(s)\n",
    "tokens = tokenizer.tokenize(s)\n",
    "print(encoding)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = [\n",
    "    (\"save/2023-04-05_SavedModels/checkpoints/2023-04-05_10:03:30_ModelGerman/2023-04-05_14:54:56_TVAE_RegTrue/model.pt\", 3, DatasetWordPiece(large=False, max_length=128), 14779805221749554585, \"German\"),\n",
    "    (\"save/2023-04-05_SavedModels/checkpoints/2023-04-05_10:05:44_ModelWiki/2023-04-05_10:05:44_TVAE_RegTrue/model.pt\", 3, DatasetWordPiece(large=True, max_length=128), 6003809420069737480, \"Wikipedia\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for processing batch data\n",
    "fake_label = torch.IntTensor([[1]])\n",
    "batch_size = 64\n",
    "\n",
    "for path_model, nlayers, dataset, seed, name in path_models:\n",
    "    model = TVAE(ntoken=dataset.vocab_size, nlayers=nlayers)\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "    # model.cuda()\n",
    "    model.eval()\n",
    "    trainer = TVAETrainer(dataset=dataset, model=model)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    _, dataset_val = torch.utils.data.random_split(\n",
    "        dataset, [0.8, 0.2], generator=generator\n",
    "    )\n",
    "    data_loader = DataLoader(dataset_val, batch_size=batch_size)\n",
    "\n",
    "    # for num, batch in tqdm(enumerate(data_loader), leave=False, total=len(data_loader), desc=f\"Sentences\"):\n",
    "    for num, batch in enumerate(data_loader):\n",
    "        # t = torch.Tensor(dataset.encode())\n",
    "        # t = t.view(1, -1).long()\n",
    "        # batch = trainer.process_batch_data((t, fake_label))\n",
    "        data = trainer.process_batch_data(batch)\n",
    "\n",
    "        d = {}\n",
    "        for i, k in enumerate([\"src\", \"tgt\", \"tgt_true\", \"tgt_mask\", \"memory_mask\", \"src_key_padding_mask\", \"tgt_key_padding_mask\", \"labels\"]):\n",
    "            d[k] = data[i]\n",
    "\n",
    "\n",
    "        src=d[\"src\"]\n",
    "        tgt=d[\"tgt\"]\n",
    "        tgt_mask=d[\"tgt_mask\"]\n",
    "        memory_mask=d[\"memory_mask\"]\n",
    "        src_key_padding_mask=d[\"src_key_padding_mask\"]\n",
    "        tgt_key_padding_mask=d[\"tgt_key_padding_mask\"]\n",
    "\n",
    "        z_dist = model.encode(src, src_key_padding_mask)\n",
    "        z_tilde, z_prior, prior_dist = model.reparametrize(z_dist)\n",
    "        \n",
    "        logits = model.decode(\n",
    "            z_tilde=z_tilde,\n",
    "            tgt=tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        out_tokens = torch.argmax(logits, dim=-1)\n",
    "        out_tokens = [int(i) for i in list(out_tokens.data.to(\"cpu\")[0])]\n",
    "        true_tokens = list(d[\"tgt_true\"][0].cpu().numpy())\n",
    "        print(\"1\", dataset.tokenizer.decode(out_tokens))\n",
    "        print(\"2\", dataset.tokenizer.decode(true_tokens))\n",
    "        # print(acc(logits, d[\"tgt_true\"]))\n",
    "        # print(true_tokens)\n",
    "        # print(out_tokens)\n",
    "        if num>5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_interpolations(self, latent_code, dim=0, num_points=10):\n",
    "    x = torch.linspace(-4.0, 4.0, num_points)\n",
    "    z = to_cuda_variable(torch.from_numpy(latent_code))\n",
    "    z = z.repeat(num_points, 1)\n",
    "    z[:, dim] = x.contiguous()\n",
    "    outputs = torch.sigmoid(self.model.decode(z))\n",
    "    interp = make_grid(outputs.cpu(), nrow=num_points, pad_value=1.0)\n",
    "    return interp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f140f8b3d41bfe26436db86396a4a983fe1f68caf3529c53ccd2c476030458a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
