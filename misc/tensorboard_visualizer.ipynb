{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.data import TFRecordDataset\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "p = pprint.PrettyPrinter(indent=4)\n",
    "os.chdir(Path(os.environ[\"MASTER\"]))\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Taken from https://gist.github.com/laszukdawid/62656cf7b34cac35b325ba21d46ecfcd\n",
    "\n",
    "    https://laszukdawid.com/blog/2021/01/26/parsing-tensorboard-data-locally/\n",
    "    \n",
    "    Updated with https://stackoverflow.com/questions/58248787/reading-tf2-summary-file-with-tf-data-tfrecorddataset#58314091\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_tfevent(filepath):\n",
    "    return pd.DataFrame([\n",
    "        parse_tfevent(e) for e in summary_iterator(filepath) if len(e.summary.value)\n",
    "    ])\n",
    "\n",
    "\n",
    "def parse_tfevent(tfevent):\n",
    "    return dict(\n",
    "        wall_time=tfevent.wall_time,\n",
    "        name=tfevent.summary.value[0].tag,\n",
    "        step=tfevent.step,\n",
    "        value=float(tfevent.summary.value[0].simple_value),\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_tb_data(root_dir, sort_by=None):\n",
    "    \"\"\"Convert local TensorBoard data into Pandas DataFrame.\n",
    "\n",
    "    Function takes the root directory path and recursively parses\n",
    "    all events data.    \n",
    "    If the `sort_by` value is provided then it will use that column\n",
    "    to sort values; typically `wall_time` or `step`.\n",
    "\n",
    "    *Note* that the whole data is converted into a DataFrame.\n",
    "    Depending on the data size this might take a while. If it takes\n",
    "    too long then narrow it to some sub-directories.\n",
    "\n",
    "    Paramters:\n",
    "        root_dir: (str) path to root dir with tensorboard data.\n",
    "        sort_by: (optional str) column name to sort by.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame with [wall_time, name, step, value] columns.\n",
    "\n",
    "    \"\"\"\n",
    "    columns_order = ['wall_time', 'name', 'step', 'value']\n",
    "\n",
    "    out = []\n",
    "    for (root, _, filenames) in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if \"events.out.tfevents\" not in filename:\n",
    "                continue\n",
    "            file_full_path = os.path.join(root, filename)\n",
    "            out.append(convert_tfevent(file_full_path))\n",
    "\n",
    "    # Concatenate (and sort) all partial individual dataframes\n",
    "    all_df = pd.concat(out)[columns_order]\n",
    "    if sort_by is not None:\n",
    "        all_df = all_df.sort_values(sort_by)\n",
    "\n",
    "    return all_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(input_dir, output_dir):\n",
    "    df = convert_tb_data(input_dir)\n",
    "    return (f\"{output_dir.parent.name}_{output_dir.name}\", df)\n",
    "\n",
    "\n",
    "def get_file_list():\n",
    "    root_path = Path(os.environ[\"MASTER\"], \"save\")\n",
    "    dirs = [dir for dir in root_path.iterdir()]\n",
    "\n",
    "    l = []\n",
    "    for dir in dirs:\n",
    "        for input_dir in Path(dir, \"runs\").iterdir():\n",
    "            l.extend(input_dir.iterdir())\n",
    "    return l\n",
    "\n",
    "def filter_list(path_list, filters):\n",
    "    l = []\n",
    "    for f in filters:\n",
    "        l.extend(filter(lambda x: f in str(x), path_list))\n",
    "    return l\n",
    "\n",
    "def get_dataframes(input_dirs):\n",
    "    l = []\n",
    "    for input_dir in input_dirs:\n",
    "        output_dir = Path(os.environ[\"MASTER\"],\n",
    "                          \"results\", input_dir.parents[2].name, input_dir.name)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir(parents=True)\n",
    "        l.append((input_dir, output_dir))\n",
    "    with Pool() as pool:\n",
    "        result = pool.starmap(parallel, l)\n",
    "    return {k: v for k, v in result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = get_file_list()\n",
    "filters= [\n",
    "    \"2023-02-11_Word_level\",\n",
    "    \"2023-02-11_Sentence_level\"\n",
    "]\n",
    "# filtered_l = filter_list(l, filters)\n",
    "# d = get_dataframes(filtered_l)\n",
    "d=get_dataframes(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.pprint()\n",
    "all_keys = [\n",
    "    'Disentanglement/Interpretability',\n",
    "    'Disentanglement/Mutual Information Gap',\n",
    "    'Disentanglement/Separated Attribute Predictability',\n",
    "    \"Disentanglement/Spearman's Rank Correlation\",\n",
    "    'accuracy/training',\n",
    "    'accuracy/validation',\n",
    "    'loss_KLD/training',\n",
    "    'loss_KLD/validation',\n",
    "    'loss_KLD_batchwise/training',\n",
    "    'loss_KLD_batchwise/validation',\n",
    "    'loss_KLD_unscaled/training',\n",
    "    'loss_KLD_unscaled/validation',\n",
    "    'loss_KLD_unscaled_batchwise/training',\n",
    "    'loss_KLD_unscaled_batchwise/validation',\n",
    "    'loss_reconstruction/training',\n",
    "    'loss_reconstruction/validation',\n",
    "    'loss_regularization/training',\n",
    "    'loss_regularization/validation',\n",
    "    'loss_sum/training',\n",
    "    'loss_sum/validation'\n",
    "]\n",
    "\n",
    "p.pprint(list(d.keys()))\n",
    "\n",
    "for title in all_keys:\n",
    "    for key, value in d.items():\n",
    "        if \"RegTrue\" in key:\n",
    "            marker=\".\"\n",
    "        else:\n",
    "            marker=\"|\"\n",
    "        if \"Word_level\" in key:\n",
    "            color=\"red\"\n",
    "        else:\n",
    "            color=\"green\"\n",
    "        plt.plot(np.array(value[value[\"name\"] == title][\"value\"]), marker=marker, color=color)\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b52a59b75e5ddc235e8ae75c6f232f2b835bf44cfac57e20d283db7df608e2c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
