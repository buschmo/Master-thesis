@misc{optimus2020,
  doi       = {10.48550/ARXIV.2004.04092},
  url       = {https://arxiv.org/abs/2004.04092},
  author    = {Li, Chunyuan and Gao, Xiang and Li, Yuan and Peng, Baolin and Li, Xiujun and Zhang, Yizhe and Gao, Jianfeng},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Pati2021ARVAE,
  title     = {Attribute-based regularization of latent spaces for variational auto-encoders},
  author    = {Pati, Ashis and Lerch, Alexander},
  journal   = {Neural Computing and Applications},
  volume    = {33},
  number    = {9},
  pages     = {4429--4444},
  year      = {2021},
  publisher = {Springer}
}



@misc{Kingma2013VAE,
  doi       = {10.48550/ARXIV.1312.6114},
  url       = {https://arxiv.org/abs/1312.6114},
  author    = {Kingma, Diederik P and Welling, Max},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Auto-Encoding Variational Bayes},
  publisher = {arXiv},
  year      = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{Higgins2017BetaVAE,
  author    = {Irina Higgins and
               Lo{\"{\i}}c Matthey and
               Arka Pal and
               Christopher P. Burgess and
               Xavier Glorot and
               Matthew M. Botvinick and
               Shakir Mohamed and
               Alexander Lerchner},
  title     = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational
               Framework},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
  timestamp = {Tue, 26 Apr 2022 19:45:27 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HigginsMPBGBML17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Coster2011SimpleWikipedia,
  title     = {{S}imple {E}nglish {W}ikipedia: A New Text Simplification Task},
  author    = {Coster, William  and
               Kauchak, David},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P11-2117},
  pages     = {665--669}
}


@online{spacy,
  title   = {spaCy: Industrial-Strength Natural Language Processing in Python},
  author  = {{Explosion AI}},
  url     = {https://spacy.io/},
  year    = 2022,
  urldate = {2022-01-10}
}

  @misc{enwiki:1093694237,
  author = {{Wikipedia contributors}},
  title  = {Skagerrak --- {Wikipedia}{,} The Free Encyclopedia},
  year   = {2022},
  url    = {https://en.wikipedia.org/w/index.php?title=Skagerrak&oldid=1093694237},
  note   = {[Online; accessed 9-October-2022]}
}


##### Similarity measurements #####

@inproceedings{interpretability,
  title     = {Discovering Interpretable Representations for Both Deep Generative and Discriminative Models},
  author    = {Adel, Tameem and Ghahramani, Zoubin and Weller, Adrian},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {50--59},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  month     = {10--15 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v80/adel18a/adel18a.pdf},
  url       = {https://proceedings.mlr.press/v80/adel18a.html},
  abstract  = {Interpretability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretability. However, this may reduce accuracy, and is not applicable to already trained models. We propose two interpretability frameworks. First, we provide an interpretable lens for an existing model. We use a generative model which takes as input the representation in an existing (generative or discriminative) model, weakly supervised by limited side information. Applying a flexible and invertible transformation to the input leads to an interpretable representation with no loss in accuracy. We extend the approach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what "interpretable" means. Our second framework relies on joint optimization for a representation which is both maximally informative about the side information and maximally compressive about the non-interpretable data factors. This leads to a novel perspective on the relationship between compression and regularization. We also propose a new interpretability evaluation metric based on our framework. Empirically, we achieve state-of-the-art results on three datasets using the two proposed algorithms.}
}

@inproceedings{MIG-MutualInformationGap,
  author    = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Isolating Sources of Disentanglement in Variational Autoencoders},
  url       = {https://proceedings.neurips.cc/paper/2018/file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf},
  volume    = {31},
  year      = {2018}
}

@misc{SAP-SeparatedAttributePredictability,
  doi       = {10.48550/ARXIV.1711.00848},
  url       = {https://arxiv.org/abs/1711.00848},
  author    = {Kumar, Abhishek and Sattigeri, Prasanna and Balakrishnan, Avinash},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Variational Inference of Disentangled Latent Concepts from Unlabeled Observations},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Spearman1904Spearman,
  title   = {Spearman’s rank correlation coefficient},
  author  = {Spearman, Charles},
  journal = {Amer. J. Psychol},
  volume  = {15},
  number  = {72-101},
  pages   = {16},
  year    = {1904}
}

@article{Pearson1896correlationcoefficient,
  title     = {VII. Mathematical contributions to the theory of evolution.—III. Regression, heredity, and panmixia},
  author    = {Pearson, Karl},
  journal   = {Philosophical Transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character},
  number    = {187},
  pages     = {253--318},
  year      = {1896},
  publisher = {The Royal Society London}
}

##### VAE Architectures #####
@misc{Cho2014RNN-VAE,
  doi       = {10.48550/ARXIV.1406.1078},
  url       = {https://arxiv.org/abs/1406.1078},
  author    = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords  = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
  publisher = {arXiv},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Wang2019T-VAE,
  author    = {Wang, Ke and Hua, Hang and Wan, Xiaojun},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation},
  url       = {https://proceedings.neurips.cc/paper/2019/file/8804f94e16ba5b680e239a554a08f7d2-Paper.pdf},
  volume    = {32},
  year      = {2019}
}



##### German Simplification #####

@article{suter2016rule,
  booktitle = {13th Conference on Natural Language Processing (KONVENS 2016)},
  month     = {September},
  title     = {Rule-based Automatic Text Simplification for German},
  author    = {Julia Suter and Sarah Ebling and Martin Volk},
  publisher = {s.n.},
  year      = {2016},
  language  = {english},
  url       = {https://doi.org/10.5167/uzh-128601}
}


@incollection{Säuberli2020Benchmarking,
  author    = {Andreas S{\"a}uberli and Sarah Ebling and Martin Volk},
  booktitle = {Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)},
  editor    = {Nuria Gala and Rodrigo Wilkens},
  address   = {Marseille},
  title     = {Benchmarking Data-driven Automatic Text Simplification for German},
  publisher = {European Language Resources Association},
  pages     = {41--48},
  year      = {2020},
  language  = {english},
  url       = {https://doi.org/10.5167/uzh-188569},
  abstract  = {Automatic text simplification is an active research area, and there are first systems for English, Spanish, Portuguese, and Italian. For German, no data-driven approach exists to this date, due to a lack of training data. In this paper, we present a parallel corpus of news items in German with corresponding simplifications on two complexity levels. The simplifications have been produced according to a well-documented set of guidelines. We then report on experiments in automatically simplifying the German news items using state-of-the-art neural machine translation techniques. We demonstrate that despite our small parallel corpus, our neural models were able to learn essential features of simplified language, such as lexical substitutions, deletion of less relevant words and phrases, and sentence shortening.}
}


@inproceedings{Mallinson2020ZeroShot,
  title     = {Zero-Shot Crosslingual Sentence Simplification},
  author    = {Mallinson, Jonathan  and
               Sennrich, Rico  and
               Lapata, Mirella},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.415},
  doi       = {10.18653/v1/2020.emnlp-main.415},
  pages     = {5109--5126},
  abstract  = {Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English. We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks. A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification). Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.}
}


@inproceedings{Spring2021ExploringGerman,
  booktitle = {International Conference on Recent Advances in Natural Language Processing (RANLP 2021)},
  month     = {September},
  title     = {Exploring German Multi-Level Text Simplification},
  author    = {Nicolas Spring and Annette Rios and Sarah Ebling},
  publisher = {ACL Anthology},
  year      = {2021},
  pages     = {1339--1349},
  language  = {english},
  url       = {https://doi.org/10.5167/uzh-209615},
  abstract  = {We report on experiments in automatic text simplification (ATS) for German with multiple simplification levels along the Common European Framework of Reference for Languages (CEFR), simplifying standard German into levels A1, A2 and B1. For that purpose, we investigate the use of source labels and pretraining on standard German, allowing us to simplify standard language to a specific CEFR level. We show that these approaches are especially effective in low-resource scenarios, where we are able to outperform a standard transformer baseline. Moreover, we introduce copy labels, which we show can help the model make a distinction between sentences that require further modifications and sentences that can be copied as-is.}
}



##### Disentanglement #####

@inproceedings{Cao2020ExpertiseStyle,
  title     = {Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen},
  author    = {Cao, Yixin  and
               Shui, Ruihao  and
               Pan, Liangming  and
               Kan, Min-Yen  and
               Liu, Zhiyuan  and
               Chua, Tat-Seng},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.100},
  doi       = {10.18653/v1/2020.acl-main.100},
  pages     = {1061--1071},
  abstract  = {The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions. The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/.}
}


@inproceedings{John2019DisentangledRepresentation,
  title     = {Disentangled Representation Learning for Non-Parallel Text Style Transfer},
  author    = {John, Vineet  and
               Mou, Lili  and
               Bahuleyan, Hareesh  and
               Vechtomova, Olga},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P19-1041},
  doi       = {10.18653/v1/P19-1041},
  pages     = {424--434},
  abstract  = {This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches.}
}


@inproceedings{Bao2019GeneratingSentences,
  title     = {Generating Sentences from Disentangled Syntactic and Semantic Spaces},
  author    = {Bao, Yu  and
               Zhou, Hao  and
               Huang, Shujian  and
               Li, Lei  and
               Mou, Lili  and
               Vechtomova, Olga  and
               Dai, Xin-yu  and
               Chen, Jiajun},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P19-1602},
  doi       = {10.18653/v1/P19-1602},
  pages     = {6008--6019},
  abstract  = {Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE{'}s latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.}
}

@inproceedings{Nangi2021Counterfactuals,
  title     = {Counterfactuals to Control Latent Disentangled Text Representations for Style Transfer},
  author    = {Nangi, Sharmila Reddy  and
               Chhaya, Niyati  and
               Khosla, Sopan  and
               Kaushik, Nikhil  and
               Nyati, Harshit},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-short.7},
  doi       = {10.18653/v1/2021.acl-short.7},
  pages     = {40--48},
  abstract  = {Disentanglement of latent representations into content and style spaces has been a commonly employed method for unsupervised text style transfer. These techniques aim to learn the disentangled representations and tweak them to modify the style of a sentence. In this paper, we propose a counterfactual-based method to modify the latent representation, by posing a {`}what-if{'} scenario. This simple and disciplined approach also enables a fine-grained control on the transfer strength. We conduct experiments with the proposed methodology on multiple attribute transfer tasks like Sentiment, Formality and Excitement to support our hypothesis.}
}


##### Image-based disentanglement #####
@misc{Ding2020Guided,
  doi       = {10.48550/ARXIV.2004.01255},
  url       = {https://arxiv.org/abs/2004.01255},
  author    = {Ding, Zheng and Xu, Yifan and Xu, Weijian and Parmar, Gaurav and Yang, Yang and Welling, Max and Tu, Zhuowen},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Guided Variational Autoencoder for Disentanglement Learning},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Rhodes2021Local,
  author    = {Rhodes, Travers and Lee, Daniel},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {22708--22719},
  publisher = {Curran Associates, Inc.},
  title     = {Local Disentanglement in Variational Auto\-/Encoders Using Jacobian L\_1 Regularization},
  url       = {https://proceedings.neurips.cc/paper/2021/file/bfd2308e9e75263970f8079115edebbd-Paper.pdf},
  volume    = {34},
  year      = {2021}
}




##### Data sets #####
@misc{Toborek2022SimpleGermanCorpus,
  doi       = {10.48550/ARXIV.2209.01106},
  url       = {https://arxiv.org/abs/2209.01106},
  author    = {Toborek, Vanessa and Busch, Moritz and Boßert, Malte and Bauckhage, Christian and Welke, Pascal},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {A New Aligned Simple German Corpus},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}
